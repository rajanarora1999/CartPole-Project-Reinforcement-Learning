{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01314811, -0.04619591,  0.00447113, -0.03369146])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Episode 1/20 High Score 19\n",
      "Game Episode 2/20 High Score 20\n",
      "Game Episode 3/20 High Score 12\n",
      "Game Episode 4/20 High Score 17\n",
      "Game Episode 5/20 High Score 18\n",
      "Game Episode 6/20 High Score 11\n",
      "Game Episode 7/20 High Score 21\n",
      "Game Episode 8/20 High Score 13\n",
      "Game Episode 9/20 High Score 14\n",
      "Game Episode 10/20 High Score 17\n",
      "Game Episode 11/20 High Score 9\n",
      "Game Episode 12/20 High Score 25\n",
      "Game Episode 13/20 High Score 11\n",
      "Game Episode 14/20 High Score 15\n",
      "Game Episode 15/20 High Score 12\n",
      "Game Episode 16/20 High Score 31\n",
      "Game Episode 17/20 High Score 46\n",
      "Game Episode 18/20 High Score 41\n",
      "Game Episode 19/20 High Score 18\n",
      "Game Episode 20/20 High Score 24\n",
      "All 20 episodes done\n"
     ]
    }
   ],
   "source": [
    "for e in range(20):\n",
    "    obs=env.reset()\n",
    "    for t in range(50):\n",
    "        env.render()\n",
    "        obs,reward,done,other=env.step(env.action_space.sample())\n",
    "        if done:\n",
    "            print(\"Game Episode {}/{} High Score {}\".format(e+1,20,t))\n",
    "            break\n",
    "env.close()\n",
    "print(\"All 20 episodes done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "class Agent :\n",
    "    def __init__(self,state_size,action_size):\n",
    "        self.state_size=state_size\n",
    "        self.action_size=action_size\n",
    "        self.memory=deque(maxlen=2000)\n",
    "        self.gamma=0.95 #discount Error\n",
    "        #in start epsilon is 100% i.e we trust exploration(random decison) more than past knowledge as in the start we dont have any past knowledge\n",
    "        self.epsilon=1.0\n",
    "        self.epsilon_decay=0.995\n",
    "        #after each game we decrease the trust on exploration by 5% and also start learning by past knowledge\n",
    "        self.epsilon_min=0.01\n",
    "        #even after 100000 games we should keep some factor for exploration\n",
    "        self.learning_rate=0.001\n",
    "        #learning rate for neural network\n",
    "        self.model=self._create_model()\n",
    "    \n",
    "    def _create_model(self):\n",
    "        model=Sequential()\n",
    "        model.add(Dense(24,input_dim=self.state_size,activation='relu'))\n",
    "        model.add(Dense(24,activation='relu'))\n",
    "        model.add(Dense(self.action_size,activation='linear'))\n",
    "        model.compile(loss='mse',optimizer=Adam(lr=0.001))\n",
    "        return model\n",
    "    def remember(self,state,action,reward,next_state,done):\n",
    "        self.memory.append((state,action,reward,next_state,done))\n",
    "        \n",
    "                    \n",
    "    def act(self,state):\n",
    "        if np.random.randn()<=self.epsilon:\n",
    "            #take a random action(exploration)\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "        #ask the neural network to tell a suitable action\n",
    "            #prediction is of form [[action_size]]\n",
    "            #so we take 0 index to get the main list\n",
    "            return np.argmax(self.model.predict(state)[0])\n",
    "    def train(self,batch_size=32):\n",
    "        #train using a replay buffer\n",
    "        #pick samples from the memory\n",
    "        minibatch=random.sample(self.memory,batch_size)\n",
    "        for experience in minibatch:\n",
    "            state,action,reward,next_state,done=experience\n",
    "            #train using X & Y (X-> state) & (Y->expected reward from this stage)\n",
    "            #predict the expected reward using bellman's equation\n",
    "            if not done:\n",
    "                #game not over\n",
    "                target=reward+self.gamma*np.amax(self.model.predict(next_state)[0])\n",
    "            else:\n",
    "                target=reward\n",
    "            target_f=self.model.predict(state)\n",
    "            target_f[0][action]=target\n",
    "            \n",
    "            self.model.fit(state,target_f,epochs=1,verbose=0)\n",
    "            if self.epsilon >self.epsilon_min:\n",
    "                self.epsilon*=self.epsilon_decay\n",
    "        \n",
    "    def load(self,name):\n",
    "        self.model.load_weights(name)\n",
    "    def save(self,name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes=1000\n",
    "batch_size=32\n",
    "output_dir='cartpole_model/'\n",
    "state_size=4\n",
    "agent=Agent(state_size=4,action_size=2)\n",
    "done=False\n",
    "for e in range(n_episodes):\n",
    "    state=env.reset()\n",
    "    state=np.reshape(state,[1,state_size])\n",
    "    for time in range(500):\n",
    "        env.render()\n",
    "        action=agent.act(state)\n",
    "        #action is 0 or 1\n",
    "        next_state,reward,done,other_info=env.step(action)\n",
    "        reward=reward if not done else -10\n",
    "        next_state=np.reshape(next_state,[1,state_size])\n",
    "        agent.remember(state,action,reward,next_state,done)\n",
    "        if done:\n",
    "            print(\"Game Episode {}/{} High Score {}\".format(e+1,n_episodes,time))\n",
    "            break\n",
    "        #train the model\n",
    "        if len(agent.memory)>batch_size:\n",
    "            agent.train(batch_size)\n",
    "        if e%50==0:\n",
    "            agent.save(output_dir+\"weights_\"+'{:04d}'.format(e)+'.hdf5')\n",
    "print(\"Deep Q Learner Model trained\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
